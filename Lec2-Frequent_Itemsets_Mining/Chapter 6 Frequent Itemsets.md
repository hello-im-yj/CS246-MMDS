# Chapter 6 : Frequent Itemsets

the discovery of frequent itemsets =â€œassociation rulesâ€ë¥¼ ë°œê²¬í•˜ëŠ” ê²ƒ!

- ***Course Outline***
    
    [https://www.youtube.com/watch?v=2NyZmnuIicw&list=PLoCMsyE1cvdVnCgHk43vRy7PVTVWJ6WVR&index=2](https://www.youtube.com/watch?v=2NyZmnuIicw&list=PLoCMsyE1cvdVnCgHk43vRy7PVTVWJ6WVR&index=2)
    
    1) â€œmarket-basketâ€ model
    
    2) First: Define
    
    Frequent itemsets
    Association rules:
    Confidence, Support, Interestingness
    
    3) Then: Algorithms for finding frequent itemsets
    
    Finding frequent pairs
    A-Priori algorithm
    PCY algorithm
    

# 6.1. THE MARKET-BASKET MODEL

---

- *Baskets* = â€œ**transactions**â€
- items $\subset$ Basket

## 6.1.1 Definition of Frequent Itemsets

<aside>
ğŸ“ â€œ*frequent*â€

â‡’ $s$, called the *support threshold*

I : set of items â†’ support for I : Iê°€ í¬í•¨ëœ Basketì˜ ìˆ˜

if support for I > $s$ : IëŠ” ë¹ˆë²ˆí•˜ë‹¤(frequent)!

</aside>

Example 6.1 :

![Untitled](Chapter%206%20Frequent%20Itemsets%206db629eaf08b4ab09f9a62f0dbb71876/Untitled.png)

- Since the empty set is a subset of any set, the support for âˆ… is 8.â‡’ í•˜ì§€ë§Œ ì¼ë°˜ì ìœ¼ë¡œ ê³µì§‘í•©ì€ ì•„ë¬´ê²ƒë„ ë§í•´ì£¼ì§€ ì•Šê¸° ë•Œë¬¸ì— ì‹ ê²½ì“°ì§€ X
- Example) â€œDogâ€:(5)ë²ˆì„ ì œì™¸ ëª¨ë“  basketì—ì„œ ë“±ì¥ â‡’ â€œDogâ€ì˜ supportëŠ” 7
- Suppose that we set our threshold at $s$ = 3 : Then there are five frequent singleton itemsets: {dog}, {cat}, {and}, {a}, and {training}

- doubletons
    
    ![Untitled](Chapter%206%20Frequent%20Itemsets%206db629eaf08b4ab09f9a62f0dbb71876/Untitled%201.png)
    
    - There are five frequent doubletons if s = 3; they are
    {dog, a} {dog, and} {dog, cat} {cat, a} {cat, and}
        - Each appears at least three times

- frequent tripleì€ frequent doubletoneì˜ ì¡°í•©ìœ¼ë¡œë§Œ ê°€ëŠ¥
- í•´ë‹¹ ì˜ˆì œì—ì„œëŠ” í•˜ë‚˜ì˜ frequent tripleë§Œ ì¡´ì¬í•˜ë¯€ë¡œ,no frequent quadruples or larger sets.

## 6.1.2 Applications of Frequent Itemsets

*: If someone buys diaper and milk, then he/she is likely to buy beer.*

1. *Related concepts:* Let 1)*items â†’ words*, and 2)*baskets â†’ documents* (e.g., Web pages, blogs, tweets). 

- stopwordë¥¼ ì œì™¸í•˜ë©´, ê³µí†µ ê°œë…ì„ ë‚˜íƒ€ë‚´ëŠ” ë‘ê°œì˜ ë‹¨ì–´ ìŒì´ ìì£¼ ë°œê²¬ë  ìˆ˜ ìˆìŒ.

2. *Plagiarism:* Let 1)the *items â†’ documents* and 2)the *baskets â†’ sentences. :* An item/document is â€œinâ€ a basket/sentence if the sentence is in the document.

- we should remember that the relationship between items and baskets is an arbitrary many-many relationship : "in"ì€ "~ì˜ ì¼ë¶€"ë¼ëŠ” ì „í†µì ì¸ ì˜ë¯¸ë¥¼ ê°€ì§ˆ í•„ìš”ê°€ ì—†ìŒ.
- ì—¬ëŸ¬ ë¬¸ì¥ì„ ê³µìœ í•˜ëŠ” ë‘ ê°œì˜ ë¬¸ì„œ â†’ í‘œì ˆì˜ ì¢‹ì€ ì§€í‘œ!

3. *Biomarkers:* Let 1) the *items* be of two types â€“ *biomarkers* such as genes or blood proteins, and *diseases.* 2) Each *basket* is the set of data about a *patient*: their genome and blood-chemistry analysis, as well as their medical history of disease. 

- ì§ˆë³‘ ê²€ì‚¬ë¡œ í™œìš© ê°€ëŠ¥

## 6.1.3 Association Rules

: extracting frequent sets of items from data = *association rules*ë¼ê³  ë¶€ë¥´ëŠ” **if-then ê·œì¹™**ì˜ ëª¨ìŒìœ¼ë¡œ í‘œì‹œ 

*** *association rules***

- form : $I â†’ j$, where $I$ : set of items, $j$ : an item
- $\{i_1, i_2,â€¦,i_k\} â†’ j$ means: â€œ**if** a basket contains all of $i_1,â€¦,i_k$ **then** it is likely to contain $j$â€
- In practice there are many rules, want to find **significant/interesting** ones!

- 1) Defining the ***confidence*** of the rule $I â†’ j$
    
    <aside>
    ğŸ“Œ conf($I â†’ j$) $\overset{\underset{\mathrm{def}}{}}{=}$ $\frac{\mathsf{support\ } (I âˆª \{j\})} {\mathsf{support\ } (I)}$
    
    </aside>
    
    : $j$ê°€ í¬í•¨ë˜ëŠ” ëª¨ë“  basket $I$ ì˜ ë¹„ìœ¨
    
    = support(I) is given â†’ how often does $j$ appear next to it?
    
    $= P(j|I) = \frac{P(I,j)}{P(I)}$ 
    
    Example) Fig 6.1ì—ì„œ 
    The confidence of the rule {cat, dog} â†’ and = 3/5.
    
    - {cat, dog}ì´ ì–¸ê¸‰ëœ basketì˜ support = 5
    - {cat, dog}ì´ and ì™€ í•¨ê»˜ ì–¸ê¸‰ëœ basket = ì¦‰, {cat, dog, and}ì˜ support = 3
- $I$ì˜ supportê°€ ë§¤ìš° í¬ë‹¤ë©´, confidence í•˜ë‚˜ë§Œìœ¼ë¡œë„ ë§¤ìš° useful í•  ìˆ˜ ìˆìŒ.
- However, Iê°€ $j$ì— (ì–´ë–»ê²Œë“ ) ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì‹¤ì œ ê´€ê³„ì—ì„œëŠ” Association Ruleì´ ë” ì¤‘ìš”í•¨.
- 2) Thus, define the ***interest*** of an association rule $I â†’ j$ : (rule $I â†’ j$ì˜ confidence)ì™€ ($j$ë¥¼ í¬í•¨í•˜ëŠ” basketì˜ ë¹„ìœ¨)ì˜ ì°¨ì´
    
    <aside>
    ğŸ“Œ $Interest(I â†’ j) =| conf(I â†’ j) - Pr[j]|$
    
    </aside>
    
    - if Iê°€ jì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•ŠëŠ”ë‹¤ë©´,
        
        $$
        
        {I\mathsf{ì™€}\ j\mathsf{ë¥¼}\ \mathsf{í¬í•¨í•˜ëŠ”\ basket} \over I\mathsf{ë¥¼\ í¬í•¨í•˜ëŠ”\ basket}} = {j\mathsf{ë¥¼\ í¬í•¨í•˜ëŠ”\ basket} \over \mathsf{ëª¨ë“ \ basket}}
        $$
        
        â†’ ruleì˜ interest = 0 
        
        - Example) Iê°€ ìˆìœ¼ë©´ í•­ìƒ Jê°€ ë”°ë¼ì˜¤ëŠ” ê²½ìš° â†’ not very interesting!
    - ruleì´ high(+) interest = basketì˜ Iê°€ â†’ jê°€ ì¡´ì¬í•˜ëŠ” ê²ƒì„ ìœ ë°œí•¨
        - the rule $\{diapers\} â†’ beer$ has high interest.
    - ruleì´ highly negative(-) interest = basketì˜ Iê°€ â†’ jê°€ ì¡´ì¬í•˜ëŠ” ê²ƒì„ ì–µì œí•¨ì„ ì•Œ ìˆ˜ ìˆìŒ.
        - the rule $\{pepsi\} â†’ coke$ can be expected to have negative interest.
    - interestê°€ ë§¤ìš° ë‚®ê±°ë‚˜ ë§¤ìš° ë†’ì€ ê²½ìš° â†’  ë‘˜ë‹¤ interesting!

## 6.1.4 Finding Association Rules with High Confidence

basketì˜ í•©ë¦¬ì ì¸ ì •ë„(reasonable fraction)ì— ì ìš©ë˜ëŠ” Association Rules $I â†’ j$ë¥¼ ì°¾ê¸° ìœ„í•´ì„œëŠ”, 

1) Iì˜ supportê°€ ìƒë‹¹íˆ(reasonably) ë†’ì•„ì•¼ í•œë‹¤. 

- ì˜¤í”„ë¼ì¸ ë§¤ì¥ì—ì„œì˜ ë§ˆì¼€íŒ…ê³¼ ê°™ì´ ì‹¤ì œë¡œ â€œreasonably highâ€í•œ ë¹„ìœ¨ì€ ì¢…ì¢… ë°”êµ¬ë‹ˆì˜ ì•½ 1%

2) ruleì˜ confidence ìƒë‹¹íˆ ë†’ì•„ì•¼ í•œë‹¤. 

- usually above 0.5 = 50%, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ê·œì¹™ì´ ì‹¤ì œ íš¨ê³¼ê°€ ê±°ì˜ X
- ê²°ê³¼ì ìœ¼ë¡œ ì§‘í•© $I âˆª \{j\}$ë„ ìƒë‹¹íˆ ë†’ì€ supportë¥¼ ê°€ì ¸ì•¼ í•¨.

<aside>
ğŸ’¡ Problem: Find all association rules with support â‰¥s and confidence â‰¥c

</aside>

â†’ This means: support($I âˆª \{j\}$) $\ge s$ â‡’ Conf $\ge c$ 

why?

- Note: Support of an association rule is the support of the set of items in the rule (left and right side)
- Hard part: Finding the frequent itemsets!

Suppose) 1. sì™€ c : given to us by user or by the data analyst.

2. threshold of support($s$)ë¥¼ ë‹¬ì„±í•˜ëŠ” ëª¨ë“  itemsetsë¥¼ ì°¾ì•˜ê³ , ê° itemsetì— ëŒ€í•œ supportë¥¼ ê³„ì‚°í–ˆë‹¤ê³  ê°€ì • â†’ ë¹ ë¥´ê²Œ conf ê³„ì‚° ê°€ëŠ¥

â†’ ë†’ì€ support && ë†’ì€ confidenceë¥¼ ê°€ì§„ Association Rulesë¥¼ ì°¾ì„ ìˆ˜ ìˆìŒ

That is) if $J$ ê°€ frequent($\ge s$)í•œ $n$ê°œì˜ itemsì„ ê°€ì§€ê³  ìˆë‹¤ë©´, 
â†’ only n possible association rules involving this set of items
= $J$ì— ì¡´ì¬í•˜ëŠ” ëª¨ë“  $j$ì— ëŒ€í•˜ì—¬ $J âˆ’ \{j\} â†’ j$ (ì´ nê°œ ì¡´ì¬)

â†’ $J$ê°€ frequent($\ge s$) ì´ë©´, $J âˆ’ \{j\}$ë„ frequent($\ge s$)

Why? If $\{i_1, i_2,â€¦, i_k\} â†’ j$ has high support and confidence, then both $\{i_1, i_2,â€¦, i_k\}$ and $\{i_1, i_2,â€¦,i_k, j\}$ will be â€œfrequentâ€

â‡’ $J$ ì™€ $J âˆ’ \{j\}$ì˜ support ë¹„ìœ¨ 
= ê·œì¹™ $J âˆ’ \{j\} â†’ j$ì˜ ì‹ ë¢°ë„ = $\frac{\mathsf{the\ support\ for\ } J-\{j\} âˆª \{j\}} {\mathsf{the\ support\ for\ } J-\{j\}}$ = $\frac{\mathsf{the\ support\ for\ } J} {\mathsf{the\ support\ for\ } J-\{j\}}$

Assumed that) there are not too many frequent itemsets and thus
not too many candidates for high-support, high-confidence association rules. â†’ ë„ˆë¬´ ë§ì€ frequent itemsetsì„ ì–»ì§€ ì•Šë„ë¡ support threshold($s$)ë¥¼ ì¡°ì •í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì !

*** How to make Algorithm ?**

Step 1) Find all frequent itemsets $I$
(we will explain this next)

Step 2) Rule generation

- For every subset $A$ of $I$, generate a rule $A â†’ I \setminus  A$
Since I is frequent, A is also frequent
    - **Variant 1**: Single pass to compute the rule confidence
    confidence(A,Bâ†’C,D) = support(A,B,C,D) / support(A,B)
    - **Variant 2**:
        - Observation: If A,B,Câ†’D is below confidence, so is A,Bâ†’C,D
            
            why? support({A,B}) > support({A,B,C})
            conf(A,B,Câ†’D) = support(A,B,C,D) / support(A,B,C)
            conf(A,Bâ†’C,D) = support(A,B,C,D) / support(A,B)
            
            $\therefore$  in every case, conf(A,B,Câ†’D) < conf(A,Bâ†’C,D)
            
        - Can generate â€œbiggerâ€ rules from smaller ones!
        Output the rules above the confidence threshold

**â†’ Finding Frequent Itemsets**

- Itemsets Computation Model
    - The true cost of mining diskresident data is usually **the number
    of disk I/Os** : diskì— ì €ì¥ëœ íŒŒì¼ì„ ì²˜ìŒ~ëê¹Œì§€ ì½ì–´ë‚´ë¦¬ëŠ” ì‹œê°„!
    - Bottleneck : main-memory
    - Question : How do we know if something cannot be frequent if we havenâ€™t counted it yet?

1) Naive Algorithm : data fileì„ í•œë²ˆ ìŠ¤ìº”í•˜ì—¬ ê° pairê°€ ëª‡ë²ˆ ë“±ì¥í•˜ëŠ”ì§€ countí•œë‹¤. â†’ impossible

**2)** Counting Pairs in Memory

![Untitled](Chapter%206%20Frequent%20Itemsets%206db629eaf08b4ab09f9a62f0dbb71876/Untitled%202.png)

Approach 1) Triangular Matrix : Dense

- ì¥ì ; 4 bytes per every pair
- ë‹¨ì ; preallocating every element

Approach 2) Triples : Sparse

- ì¥ì ; preallocating nothing
- ë‹¨ì ; 12 bytes per occuring pair

â†’ â€œhow many different pairs actually occured in the dataâ€ì— ë”°ë¼ ì‚¬ìš©í•  ë°©ë²• ê²°ì •.

- all possible pairs almost all occur : App1
- lots of items, but only few pairs tent to occur : App2

& Approach 2 beats Approach 1 if less than 1/3 of possible pairs actually occur. (ê°€ëŠ¥í•œ ìŒì˜ 1/3 ë¯¸ë§Œì´ ë°œìƒí•  ê²½ìš° App2ê°€ ë” íš¨ìœ¨ì )

ğŸ’¥ **Problem** : if we have too many items so the pairs do not fit into memory. Can we do better? â†’ A-Priori Algorithm !

# 6.2 Market Baskets and the A-Priori Algorithm

---

- `Key idea` : Monotonicity of Frequent

â‡’ if set of items $I$ê°€ ìµœì†Œ $s$ë²ˆ ë‚˜íƒ€ë‚œë‹¤ë©´, $I$ì˜ subset $J$ë„ ìµœì†Œ $s$ë²ˆ ë‚˜íƒ€ë‚¨. Jê°€ less frequent í•  ìˆ˜ ì—†ìŒ. 

ëŒ€ìš° â‡’ if item $i$ê°€ $s$ê°œì˜ basketì—ì„œ ë‚˜íƒ€ë‚˜ì§€ ì•ŠëŠ”ë‹¤ë©´(support thresholdë¥¼ ë„˜ì§€ ëª»í•œë‹¤ë©´), $i$ë¥¼ í¬í•¨í•˜ëŠ” ëª¨ë“  ì§‘í•©ì€ $s$ë¥¼ ë„˜ì„ ìˆ˜ ì—†ìŒ.

- `method` : frequent singletones to frequent pairs!

Pass 1)Read baskets and count in main memory the # of occurrences of each individual item

â†’  Items that appear $â‰¥s$ times are the **frequent items**

Pass 2) Read baskets again and keep track of the count of only those pairs where both elements are frequent (from Pass 1)

â†’ Requires memory proportional to square of **frequent items** only (not all items)

â†’ K-tupleë¡œ ì¼ë°˜í™” ê°€ëŠ¥

![Untitled](Chapter%206%20Frequent%20Itemsets%206db629eaf08b4ab09f9a62f0dbb71876/Untitled%203.png)

: single â†’ pairs â†’ triples â†’ ... k-tuples

Example) 

![Untitled](Chapter%206%20Frequent%20Itemsets%206db629eaf08b4ab09f9a62f0dbb71876/Untitled%204.png)

C3ì˜ {b,c,j} {b,m,j} {c,m,j} ëŠ” non-frequent í•  ìˆ˜ë°–ì— ì—†ìŒ

why? {b,c,j}ì—ì„œ {b,c}ì™€ {c,j}ëŠ” frequentí•˜ì§€ë§Œ, {b,j}ëŠ” frequentí•˜ì§€ ì•ŠìŒ. (by L2) 

â‡’ {b,c,j}ëŠ” frequentí•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, generateí•˜ì§€ ì•Šì•„ë„ ë¨. 

# 6.3 Handling Larger Datasets in Main Memory

---

â†’ to cut down on the size of candidate set $C_2$

## 6.3.1 PCY(Park, Chen, and Yu) Algorithm

= Also known as â€œDHP(Direct Hashing and Pruning)â€

- **Pass 1**

```sql
FOR (each basket) :
		FOR (each item in the basket) :
				add 1 to itemâ€™s count;
		<!-- new in PCY , Hashing Process -->
		FOR (each pair of items) :
				hash the pair to a bucket;
				add 1 to the count for that bucket;
```

PCY - Hash Table

- keys : Pairs
- buckets : integers(Count)

![í”„ë ˆì  í…Œì´ì…˜ 1.png](Chapter%206%20Frequent%20Itemsets%206db629eaf08b4ab09f9a62f0dbb71876/%ED%94%84%EB%A0%88%EC%A0%A0%ED%85%8C%EC%9D%B4%EC%85%98_1.png)

- **Between Passes**

![Untitled](Chapter%206%20Frequent%20Itemsets%206db629eaf08b4ab09f9a62f0dbb71876/Untitled%205.png)

Observation: 

1) If a bucket contains a frequent pair, then the bucket is surely frequent

- However, even without any frequent pair, a bucket can still be frequent â˜¹ï¸
Â§ So, we cannot use the hash to eliminate any member (pair) of a â€œfrequentâ€ bucket

2) But, for a bucket with total count less than $s$, none of its pairs can be frequent ğŸ™‚

- Pairs that hash to this bucket can be eliminated as candidates (even if the pair consists of 2 frequent items)
- Pass 2: **Only count** pairs that hash to frequent buckets

cf> hash tableì„ ë§Œë“¤ ë•Œ, (ë©”ëª¨ë¦¬ê°€ ì¶©ë¶„í•˜ë‹¤ë©´) ê°€ëŠ¥í•œ fewer collisionì„ ë§Œë“œëŠ” ê²ƒì´ ì¢‹ìŒ! 

implementation : Replace the buckets by a bit-vector!

â†’ **1** means the bucket count exceeded the support $s$(call it a frequent bucket); **0** means it did not

â†’ countë¥¼ ìœ„í•œ 4byte Integer ê°€ bit-vectorë¡œ ëŒ€ì²´ë˜ë©´ì„œ 1/32 ë©”ëª¨ë¦¬ë§Œ ì‚¬ìš© ê°€ëŠ¥!

- pass1ì—ì„œ pass2ë¡œ ë„˜ì–´ê°€ë©´ì„œ hash tableì´ frequent bucketì¸ì§€ ì—¬ë¶€($\ge s$) ë¥¼ Bitmapìœ¼ë¡œ ê¸°ë¡.

- **Pass 2**

Count all pairs {i, j} that meet the conditions for being a candidate pair:

1. Both $i$ and $j$ are frequent items
2. The pair {i, j} hashes to a bucket whose bit in the bit vector is 1 (i.e., a frequent bucket) 

â‡’ ë‘ê°€ì§€ ì¡°ê±´ì´ ëª¨ë‘ ë§Œì¡±ë˜ë©´ Tracking : 
Both conditions are *necessary* for the pair to have a chance of being frequent.

### **** Example***

![Untitled](Chapter%206%20Frequent%20Itemsets%206db629eaf08b4ab09f9a62f0dbb71876/Untitled%206.png)

**Pass 1 :**

1. Itemâ€™s Count : ê° itemì´ ëª‡ë²ˆ ë“±ì¥í•˜ëŠ”ì§€ countí•œë‹¤.
    
    * Hash Table ì•„ë‹˜
    
    *** Item Count ê²°ê³¼**
    
    | Item | Count |
    | --- | --- |
    | 1 | 5 |
    | 2 | 5 |
    | 3 | 6 |
    | 4 | 2 |
    | 5 | 4 |
    - item4ëŠ” $s$ ë¥¼ ë„˜ì§€ ëª»í•¨.
2. Make Hash Table for bucket counts
    1. step 1) ëª¨ë“  basketì˜ ê°€ëŠ¥í•œ Pairë¥¼ ìƒì„±
    2. step 2) ê° Pairë¥¼ Hash Tableì— í•´ì‹±
        
        â†’ ì—¬ê¸°ì„œëŠ” Hashing Ruleì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ 
        : Hashing a pair {i, j} to a bucket k, where k = hash(i, j) = (i + j) / 5
        
        ì˜ˆë¥¼ ë“¤ì–´, {1,2}ë¥¼ hashingí•œë‹¤ë©´ (1+2)/5 = 3 ì´ë‹ˆê¹Œ 3ë²ˆ bucketì— hashingë¨.
        
        ```
        (1, 4) and (2, 3) ->  k = 0
        (1, 5) and (2, 4) ->  k = 1
        (2, 5) and (3, 4) ->  k = 2
        (1, 2) and (3, 5) ->  k = 3
        (1, 3) and (4, 5) ->  k = 4
        ```
        
        ![Untitled](Chapter%206%20Frequent%20Itemsets%206db629eaf08b4ab09f9a62f0dbb71876/Untitled%207.png)
        
    
    *** Hash Table ê²°ê³¼**
    
    | Bucket | Count |
    | --- | --- |
    | 0 | 6 |
    | 1 | 2 |
    | 2 | 4 |
    | 3 | 4 |
    | 4 | 5 |
    - 1ë²ˆ ë²„í‚·ìœ¼ë¡œ hashingë˜ëŠ” pairëŠ” t2ì˜ (1,5)ì™€ t6ì˜ (1,5)ë§Œ ì¡´ì¬ â†’ Count : 2
    - 1ë²ˆ ë²„í‚·ì— ì†í•˜ëŠ” pairëŠ” $s$ë¥¼ ë„˜ì§€ ëª»í•¨.
    â†’ 1ë²ˆ ë²„í‚·ì— ì†í•˜ëŠ” (1,5)ì™€ (2.4)ëŠ” not frequent!

**Pass 2 :** 

Frequent items : {1,2,3,5} (By Pass 1 - itemâ€™s count)

â†’ ì´ì— ë”°ë¼ì„œ, ê°€ëŠ¥í•œ candidate pairëŠ” (1,2) (1,3) (1,5) (2,3) (2,5) (3,5)

â­ (1,5)ëŠ” íê¸° : because bucket 1 is not frequent! (By Pass 1 - hash Table)

â†’ Surviving Pairs = (1,2) (1,3) (2,3) (2,5) (3,5)

â†’ Counts of the Surviving Pairs

| Pair | Count |
| --- | --- |
| (1,2) | 2 |
| (1,3) | 4 |
| (2,3) | 4 |
| (2,5) | 3 |
| (3,5) | 2 |
- (1,2) (3,5)ëŠ” $s$ë¥¼ ë„˜ì§€ ëª»í•¨

â‡’ Result : Frequent itemsets are {1} {2} {3} {5} {1,3} {2,3} {2,5}

The MMDS book covers several other extensions beyond the PCY idea: â€œMultistageâ€ and â€œMultihashâ€

- Recommended video (starting about 10:10): [https://www.youtube.com/watch?v=AGAkNiQnbjY](https://www.youtube.com/watch?v=AGAkNiQnbjY)

# 6.4 Limited-Pass Algorithms

---

: Can we use fewer passes? ( in $\le k$ passes)

**** Frequent Itemsets in $\le 2$ Passes***

Use 2 or fewer passes for all sizes,but may miss some frequent itemsets

1. Random sampling
2. SON (Savasere, Omiecinski, and Navathe) Algorithm
3. Toivonen Algorithm

## 6.4.1 The Simple, Randomized Algorithm

: 1) Take a random sample of the market baskets 
â†’ 2) Run a-priori or one of its improvements in main memory

- ì¥ì  : Disk I/O ì‹œê°„ í•„ìš” X
- Sample sizeì— ë§ê²Œ support threshold($s$)ë¥¼ ê°ì†Œì‹œì¼œì•¼ í•¨.
    - Example) if your sample is 1/100 of the baskets, use $s/100$ as your support threshold instead of $s$.
    - Smaller threshold, e.g., $s/125$, helps catch more truly frequent itemsets (But requires more space)
- To avoid *false positives*: ì¶”ê°€ë¡œ, candidate pairsê°€ ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ì„œ frequentí•œì§€ í™•ì¸í•˜ê¸° ìœ„í•´ì„œ **second pass**ì—ì„œ ì¶”ê°€ë¡œ samplingí•œ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ê²€ì¦!

## 6.4.4 The SON Algorithm and MapReduce

: Repeatedly read small subsets of the baskets into main memory and run an in-memory algorithm to find all frequent itemsets

- Note: Sampling ë°©ë²•ê³¼ ë¹„ìŠ·í•˜ì§€ë§Œ, samplingí•˜ëŠ” ê²ƒ ì•„ë‹˜(6.1.1ê³¼ ë‹¤ë¥´ê²Œ), but ëª¨ë“  file dataë¥¼ in memory-sized **chunks**ë¡œ ìª¼ê°¬.

Path 1) An itemset becomes a candidate if it is found to be frequent in any one or more subsets of the baskets. (: make Candidate itemsets)

pass 2) count all the candidate itemsets and determine which are frequent in the entire set.(: Verify)

â†’ SON ì•Œê³ ë¦¬ì¦˜ì€ ë³‘ë ¬ ì»´í“¨íŒ… í™˜ê²½ì— ì í•©. ê° passë¥¼ MapReduce ì‘ì—…ìœ¼ë¡œ í‘œí˜„í•˜ì—¬ ë‘ ë‹¨ê³„ì˜ MapReduce-MapReduce ì‹œí€€ìŠ¤ë¡œ ì‹¤í–‰ì‹œí‚¬ ìˆ˜ ìˆìŒ. 

## 6.4.5 Toivonenâ€™s Algorithm

Pass 1) 

1. Start with a random sample
    - but lower the threshold($s$) slightly for the sample
2. Find frequent itemsets in the sample
3. construct the ***negative border***
    - Negative border : An itemset is in the negative
    border *if it is not frequent in the sample, but all its immediate subsets are frequent.*
    - Immediate subset = â€œdelete exactly one elementâ€
    
    ![Untitled](Chapter%206%20Frequent%20Itemsets%206db629eaf08b4ab09f9a62f0dbb71876/Untitled%208.png)
    

Pass 2) new sampling â†’ Count all candidate frequent itemsets from the first pass, and also count sets in their **negative border**. 

- If no itemset from the negative border turns out to be frequent, then we found all the frequent itemsets.
- What if we find that something in the negative border is frequent?
â†’ We must start over again with another sample!

â€» ëŒ€ì²´ë¡œ, Pass 1ì—ì„œëŠ” frequentí•  ìˆ˜ ìˆëŠ” candidate pairsë¥¼ ë§Œë“¤ê³  â†’ Pass 2ëŠ” candidateë“¤ì„ ì „ì²´ datasetì— ëŒ€í•´ì„œ verify í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜!